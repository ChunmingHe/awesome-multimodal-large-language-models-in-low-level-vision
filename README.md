
# <p align=center>Awesome Multimodal Large Language Models In Low-level Vision[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/ChunmingHe/awesome-multimodal-large-language-models-in-low-level-vision)</p>

<p align=center>ðŸ”¥A curated list of awesome <b>Multimodal Large Language Models(MLLMs)</b> & <b>Vision-Language Models(MLLMs)</b> in low-level vision.ðŸ”¥</p>

<p align=center>Please feel free to offer your suggestions in the Issues and pull requests to add links.</p>

<p align=center><b>[ Last updated at 2024/11/10 ]</b></p>

## Contents

- [Awesome Multimodal Large Language Models In Low-level Vision](#awesome-multimodal-large-language-models-in-low-level-vision)
  - [Contents](#contents)
  - [Latest Works Recommended](#latest-works-recommended)
  - [Awesome Papers](#awesome-papers)
  - [Related Surveys Recommended](#related-surveys-recommended)
  - [Awesome Datasets](#awesome-datasets)
  - [Benchmarks for Evaluation](#benchmarks-for-evaluation)
  - [Reference](#reference)

## <a id="latest-works-recommended">Latest Works Recommended</a>

## <a id="awesome-papers">Awesome Papers</a>

|  Title  |   Venue  |   Date   |   Code   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**PixelGaussian: Generalizable 3D Gaussian Reconstruction from Arbitrary Views**](http://arxiv.org/abs/2410.18979) <br> | arXiv | 2024-10-24 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**Task Scheduling for Efficient Inference of Large Language Models on Single Moderate GPU Systems**](http://arxiv.org/abs/2411.15715) <br> | arXiv | 2024-11-24 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**Visual Instruction Tuning**](http://arxiv.org/abs/2304.08485) <br> | arXiv | 2023-12-11 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**Chat2SVG: Vector Graphics Generation with Large Language Models and Image Diffusion Models**](http://arxiv.org/abs/2411.16602) <br> | arXiv | 2024-11-25 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**FLAME: Frozen Large Language Models Enable Data-Efficient Language-Image Pre-training**](http://arxiv.org/abs/2411.11927) <br> | arXiv | 2024-11-22 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**Image Regeneration: Evaluating Text-to-Image Model via Generating Identical Image with Multimodal Large Language Models**](http://arxiv.org/abs/2411.09449) <br> | arXiv | 2024-11-14 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**Large Language Models for Lossless Image Compression: Next-Pixel Prediction in Language Space is All You Need**](http://arxiv.org/abs/2411.12448) <br> | arXiv | 2024-11-22 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**Learning A Low-Level Vision Generalist via Visual Task Prompt**](http://arxiv.org/abs/2408.08601) <br> | arXiv | 2024-08-16 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**DegAE: A New Pretraining Paradigm for Low-Level Vision**](-) <br> | - | ----- | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**LM4LV: A Frozen Large Language Model for Low-level Vision Tasks**](http://arxiv.org/abs/2405.15734) <br> | arXiv | 2024-06-11 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**LoMAE: Low-level Vision Masked Autoencoders for Low-dose CT Denoising**](http://arxiv.org/abs/2310.12405) <br> | arXiv | 2023-10-19 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**Multimodal Attention Networks for Low-Level Vision-and-Language Navigation**](http://arxiv.org/abs/1911.12377) <br> | arXiv | 2021-07-30 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**On Domain-Specific Post-Training for Multimodal Large Language Models**](http://arxiv.org/abs/2411.19930) <br> | arXiv | 2024-11-29 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**StoryTeller: Improving Long Video Description through Global Audio-Visual Character Identification**](http://arxiv.org/abs/2411.07076) <br> | arXiv | 2024-11-11 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**EditScribe: Non-Visual Image Editing with Natural Language Verification Loops**](http://arxiv.org/abs/2408.06632) <br> | arXiv | 2024-08-13 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**MG-LLaVA: Towards Multi-Granularity Visual Instruction Tuning**](http://arxiv.org/abs/2406.17770) <br> | arXiv | 2024-06-27 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**Visually Descriptive Language Model for Vector Graphics Reasoning**](http://arxiv.org/abs/2404.06479) <br> | arXiv | 2024-10-03 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**AutomaTikZ: Text-Guided Synthesis of Scientific Vector Graphics with TikZ**](http://arxiv.org/abs/2310.00367) <br> | arXiv | 2024-01-23 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**RestoreAgent: Autonomous Image Restoration Agent via Multimodal Large Language Models**](http://arxiv.org/abs/2407.18035) <br> | arXiv | 2024-07-25 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**LLMRA: Multi-modal Large Language Model based Restoration Assistant**](http://arxiv.org/abs/2401.11401) <br> | arXiv | 2024-01-21 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**UniProcessor: A Text-Induced Unified Low-Level Image Processor**](-) <br> | Springer Nature Switzerland | 2025---- | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**An Intelligent Agentic System for Complex Image Restoration Problems**](http://arxiv.org/abs/2410.17809) <br> | arXiv | 2024-10-23 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**MLeVLM: Improve Multi-level Progressive Capabilities based on Multimodal Large Language Model for Medical Visual Question Answering**](https://aclanthology.org/2024.findings-acl.296) <br> | Association for Computational Linguistics | 2024-08-- | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**MoVA: Adapting Mixture of Vision Experts to Multimodal Context**](http://arxiv.org/abs/2404.13046) <br> | arXiv | 2024-10-31 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**A Comprehensive Study of Multimodal Large Language Models for Image Quality Assessment**](http://arxiv.org/abs/2403.10854) <br> | arXiv | 2024-07-11 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration**](https://openaccess.thecvf.com/content/CVPR2024/html/Ye_mPLUG-Owl2_Revolutionizing_Multi-modal_Large_Language_Model_with_Modality_Collaboration_CVPR_2024_paper.html) <br> | - | 2024---- | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**Prismer: A Vision-Language Model with Multi-Task Experts**](http://arxiv.org/abs/2303.02506) <br> | arXiv | 2024-01-18 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**CoLLaVO: Crayon Large Language and Vision mOdel**](http://arxiv.org/abs/2402.11248) <br> | arXiv | 2024-06-02 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and Understanding**](http://arxiv.org/abs/2406.19389) <br> | arXiv | 2024-10-01 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**Vision language models are blind**](http://arxiv.org/abs/2407.06581) <br> | arXiv | 2024-07-26 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**Visual Cue Enhancement and Dual Low-Rank Adaptation for Efficient Visual Instruction Fine-Tuning**](http://arxiv.org/abs/2411.12787) <br> | arXiv | 2024-12-02 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**ForgeryGPT: Multimodal Large Language Model For Explainable Image Forgery Detection and Localization**](http://arxiv.org/abs/2410.10238) <br> | arXiv | 2024-10-14 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**AICoderEval: Improving AI Domain Code Generation of Large Language Models**](http://arxiv.org/abs/2406.04712) <br> | arXiv | 2024-06-07 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding**](http://arxiv.org/abs/2311.08046) <br> | arXiv | 2024-04-05 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**A Comprehensive Review of Multimodal Large  Language Models: Performance and Challenges  Across Different Tasks**](https://arxiv.org/abs/2408.01319) <br> | arXiv | 2024-08-02 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**A Comprehensive Review of Multimodal Large  Language Models: Performance and Challenges  Across Different Tasks**](https://arxiv.org/abs/2408.01319) <br> | arXiv | 2024-08-02 | [-](-) | - |

## <a id="related-surveys-recommended">Related Surveys Recommended</a>

**A Survey on Large Language Models for Recommendation**<br />
arXiv 2024. [[Paper](http://arxiv.org/abs/2305.19860)] <br />Jun. 2024<br />

**A Survey of Large Language Models**<br />
arXiv 2024. [[Paper](http://arxiv.org/abs/2303.18223)] <br />Oct. 2024<br />

**From Word Vectors to Multimodal Embeddings: Techniques, Applications, and Future Directions For Large Language Models**<br />
arXiv 2024. [[Paper](http://arxiv.org/abs/2411.05036)] <br />Nov. 2024<br />

**MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs**<br />
arXiv 2024. [[Paper](http://arxiv.org/abs/2411.15296)] <br />Nov. 2024<br />

**Natural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey**<br />
arXiv 2024. [[Paper](http://arxiv.org/abs/2411.17558)] <br />Nov. 2024<br />

**Large Language Model-Brained GUI Agents: A Survey**<br />
arXiv 2024. [[Paper](http://arxiv.org/abs/2411.18279)] <br />Nov. 2024<br />

**Visual Prompting in Multimodal Large Language Models: A Survey**<br />
arXiv 2024. [[Paper](http://arxiv.org/abs/2409.15310)] <br />Sep. 2024<br />

**A Survey of Camouflaged Object Detection and Beyond**<br />
arXiv 2024. [[Paper](http://arxiv.org/abs/2408.14562)] <br />Aug. 2024<br />

**MM-LLMs: Recent Advances in MultiModal Large Language Models**<br />
arXiv 2024. [[Paper](http://arxiv.org/abs/2401.13601)] <br />May. 2024<br />

**A Survey on Benchmarks of Multimodal Large Language Models**<br />
arXiv 2024. [[Paper](http://arxiv.org/abs/2408.08632)] <br />Sep. 2024<br />

**A Survey on Multimodal Benchmarks: In the Era of Large AI Models**<br />
arXiv 2024. [[Paper](http://arxiv.org/abs/2409.18142)] <br />Sep. 2024<br />

**Multimodal Image Synthesis and Editing: The Generative AI Era**<br />
arXiv 2023. [[Paper](http://arxiv.org/abs/2112.13592)] <br />Aug. 2023<br />

**A Survey on Visual Transformer**<br />
arXiv 2023. [[Paper](http://arxiv.org/abs/2012.12556)] <br />Jul. 2023<br />

**Personalized Multimodal Large Language Models: A Survey**<br />
arXiv 2024. [[Paper](http://arxiv.org/abs/2412.02142)] <br />Dec. 2024<br />

**Explainable and Interpretable Multimodal Large Language Models: A Comprehensive Survey**<br />
arXiv 2024. [[Paper](http://arxiv.org/abs/2412.02104)] <br />Dec. 2024<br />

**A Survey on Vision-Language-Action Models for Embodied AI**<br />
arXiv 2024. [[Paper](http://arxiv.org/abs/2405.14093)] <br />Nov. 2024<br />

**A Survey on Multimodal Large Language Models**<br />
National Science Review 2024. [[Paper](https://doi.org/10.1093/nsr/nwae403)] <br />Nov. 2024<br />Nov. 2024<br />

## <a id="awesome-datasets">Awesome Datasets</a>

## <a id="benchmarks-for-evaluation">Benchmarks for Evaluation</a>

|  Name  |   Paper  |   Link   |   Notes   |
|:--------|:--------:|:--------:|:--------:|
| **Q-Bench** | [Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision](http://arxiv.org/abs/2309.14181) | [repo](https://q-future.github.io/Q-Bench) | A holistic benchmark crafted to systematically evaluate potential abilities of MLLMs on three realms: low-level visual perception, low-level visual description, and overall visual quality assessment. |
| **Q-Bench<sup>+</sup>** | [Q-Bench<sup>+</sup>: A Benchmark for Multi-modal Foundation Models on Low-level Vision from Single Images to Pairs](http://arxiv.org/abs/2402.07116) | [repo](https://github.com/Q-Future/Q-Bench) | A benchmark settings to emulate human language responses related to low-level vision: the low-level visual perception via visual question answering related to low-level attributes; and the low-level visual description, on evaluating MLLMs for low-level text descriptions. |
| **ChartInsights** | [ChartInsights: Evaluating Multimodal Large Language Models for Low-Level Chart Question Answering](http://arxiv.org/abs/2409.09748) | [repo](https://chartinsight.github.io) | - |
| **HEIE** | [HEIE: MLLM-Based Hierarchical Explainable AIGC Image Implausibility Evaluator](http://arxiv.org/abs/2411.17261) | [repo](-) | A novel MLLM-Based Hierarchical Explainable image Implausibility Evaluator. |
| **QL-Bench** | [Explore the Hallucination on Low-level Perception for MLLMs](http://arxiv.org/abs/2409.09748) | [repo](-) | - |

## <a id="reference">Reference</a>
[Awesome-Multimodal-Large-Language-Models-by-BradyFU](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)

