
# <p align=center>Awesome Multimodal Large Language Models In Low-level Vision[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/ChunmingHe/awesome-multimodal-large-language-models-in-low-level-vision)</p>

<p align=center>ðŸ”¥A curated list of awesome <b>Multimodal Large Language Models(MLLMs)</b> & <b>Vision-Language Models(MLLMs)</b> in low-level vision.ðŸ”¥</p>

<p align=center>Please feel free to offer your suggestions in the Issues and pull requests to add links.</p>

<p align=center><b>[ Last updated at 2024/11/10 ]</b></p>

## Contents

- [Awesome Multimodal Large Language Models In Low-level Vision](#awesome-multimodal-large-language-models-in-low-level-vision)
  - [Contents](#contents)
  - [Latest Works Recommended](#latest-works-recommended)
  - [Awesome Papers](#awesome-papers)
  - [Related Surveys Recommended](#related-surveys-recommended)
  - [Awesome Datasets](#awesome-datasets)
  - [Benchmarks for Evaluation](#benchmarks-for-evaluation)
  - [Reference](#reference)

## <a id="latest-works-recommended">Latest Works Recommended</a>

## <a id="awesome-papers">Awesome Papers</a>

|  Title  |   Venue  |   Date   |   Code   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**Visual Instruction Tuning**](http://arxiv.org/abs/2304.08485) <br> | arXiv | 2023-12-11 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**Chat2SVG: Vector Graphics Generation with Large Language Models and Image Diffusion Models**](http://arxiv.org/abs/2411.16602) <br> | arXiv | 2024-11-25 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**FLAME: Frozen Large Language Models Enable Data-Efficient Language-Image Pre-training**](http://arxiv.org/abs/2411.11927) <br> | arXiv | 2024-11-22 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**Image Regeneration: Evaluating Text-to-Image Model via Generating Identical Image with Multimodal Large Language Models**](http://arxiv.org/abs/2411.09449) <br> | arXiv | 2024-11-14 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/lyh-18/DegAE_DegradationAutoencoder.svg?style=social&label=Star) <br> [**DegAE: A New Pretraining Paradigm for Low-level Vision**](https://openaccess.thecvf.com/content/CVPR2023/html/Liu_DegAE_A_New_Pretraining_Paradigm_for_Low-Level_Vision_CVPR_2023_paper.html) <br> | CVPR | 2023 | [Code](https://github.com/lyh-18/DegAE_DegradationAutoencoder) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**On Domain-Specific Post-Training for Multimodal Large Language Models**](http://arxiv.org/abs/2411.19930) <br> | arXiv | 2024-11-29 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**StoryTeller: Improving Long Video Description through Global Audio-Visual Character Identification**](http://arxiv.org/abs/2411.07076) <br> | arXiv | 2024-11-11 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**EditScribe: Non-Visual Image Editing with Natural Language Verification Loops**](http://arxiv.org/abs/2408.06632) <br> | arXiv | 2024-08-13 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**MG-LLaVA: Towards Multi-Granularity Visual Instruction Tuning**](http://arxiv.org/abs/2406.17770) <br> | arXiv | 2024-06-27 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**UniProcessor: A Text-Induced Unified Low-Level Image Processor**](-) <br> | Springer Nature Switzerland | 2025---- | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**An Intelligent Agentic System for Complex Image Restoration Problems**](http://arxiv.org/abs/2410.17809) <br> | arXiv | 2024-10-23 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**MLeVLM: Improve Multi-level Progressive Capabilities based on Multimodal Large Language Model for Medical Visual Question Answering**](https://aclanthology.org/2024.findings-acl.296) <br> | Association for Computational Linguistics | 2024-08-- | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**MoVA: Adapting Mixture of Vision Experts to Multimodal Context**](http://arxiv.org/abs/2404.13046) <br> | arXiv | 2024-10-31 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration**](https://openaccess.thecvf.com/content/CVPR2024/html/Ye_mPLUG-Owl2_Revolutionizing_Multi-modal_Large_Language_Model_with_Modality_Collaboration_CVPR_2024_paper.html) <br> | - | 2024---- | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**Prismer: A Vision-Language Model with Multi-Task Experts**](http://arxiv.org/abs/2303.02506) <br> | arXiv | 2024-01-18 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**CoLLaVO: Crayon Large Language and Vision mOdel**](http://arxiv.org/abs/2402.11248) <br> | arXiv | 2024-06-02 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and Understanding**](http://arxiv.org/abs/2406.19389) <br> | arXiv | 2024-10-01 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**Vision language models are blind**](http://arxiv.org/abs/2407.06581) <br> | arXiv | 2024-07-26 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**Visual Cue Enhancement and Dual Low-Rank Adaptation for Efficient Visual Instruction Fine-Tuning**](http://arxiv.org/abs/2411.12787) <br> | arXiv | 2024-12-02 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**ForgeryGPT: Multimodal Large Language Model For Explainable Image Forgery Detection and Localization**](http://arxiv.org/abs/2410.10238) <br> | arXiv | 2024-10-14 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding**](http://arxiv.org/abs/2311.08046) <br> | arXiv | 2024-04-05 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**A Comprehensive Review of Multimodal Large Language Models: Performance and Challenges Across Different Tasks**](https://arxiv.org/abs/2408.01319) <br> | arXiv | 2024-08-02 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/PKU-YuanGroup/Chat-UniVi.svg?style=social&label=Star) <br> [**Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding**](https://openaccess.thecvf.com/content/CVPR2024/html/Jin_Chat-UniVi_Unified_Visual_Representation_Empowers_Large_Language_Models_with_Image_CVPR_2024_paper.html) <br> | CVPR | 2024-06 | [Code](https://github.com/PKU-YuanGroup/Chat-UniVi) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**Decoder-Only Transformers: The Brains Behind Generative AI, Large Language Models and Large Multimodal Models**](https://www.techrxiv.org/doi/full/10.36227/techrxiv.173198819.91727188) <br> | techrxiv | 2024-11-19 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/baaivision/Emu.svg?style=social&label=Star) <br> [**EMU: GENERATIVE PRETRAINING IN MULTIMODALITY**](https://openreview.net/forum?id=mL8Q9OOamV) <br> | OpenReview | 2024-03-15 | [Code](https://github.com/baaivision/Emu) | - |
| ![Star](https://img.shields.io/github/stars/ugorsahin/enhancing-multimodal-compositional-reasoning-of-vlm.svg?style=social&label=Star) <br> [**Enhancing Multimodal Compositional Reasoning of Visual Language Models with Generative Negative Mining**](https://openaccess.thecvf.com/content/WACV2024/html/Sahin_Enhancing_Multimodal_Compositional_Reasoning_of_Visual_Language_Models_With_Generative_WACV_2024_paper.html) <br> | WACV | 2024-01 | [Code](https://ugorsahin.github.io/enhancing-multimodal-compositional-reasoning-of-vlm.html) | - |
| ![Star](https://img.shields.io/github/stars/kohjingyu/gill.svg?style=social&label=Star) <br> [**Generating Images with Multimodal Language Models**](https://proceedings.neurips.cc/paper_files/paper/2023/hash/43a69d143273bd8215578bde887bb552-Abstract-Conference.html) <br> | NeurIPS | 2023-12 | [Code](https://github.com/kohjingyu/gill) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**Generative Cross-Modal Retrieval: Memorizing Images in Multimodal Language Models for Retrieval and Beyond**](https://arxiv.org/abs/2402.10805) <br> | arXiv | 2024-02-16 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/xinwei666/mmgenerativeir.svg?style=social&label=Star) <br> [**Generative Multi-Modal Knowledge Retrieval with Large Language Models**](https://ojs.aaai.org/index.php/AAAI/article/view/29837) <br> | AAAI | 2024-03-24 | [Code](https://github.com/xinwei666/mmgenerativeir) | - |
| ![Star](https://img.shields.io/github/stars/baaivision/emu.svg?style=social&label=Star) <br> [**Generative Multimodal Models are In-Context Learners**](https://openaccess.thecvf.com/content/CVPR2024/html/Sun_Generative_Multimodal_Models_are_In-Context_Learners_CVPR_2024_paper.html) <br> | CVPR | 2024-06 | [Code](https://github.com/baaivision/emu) | - |
| ![Star](https://img.shields.io/github/stars/kohjingyu/fromage.svg?style=social&label=Star) <br> [**Grounding Language Models to Images for Multimodal Inputs and Outputs**](https://proceedings.mlr.press/v202/koh23a.html) <br> | PMLR | 2023 | [Code](https://github.com/kohjingyu/fromage) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**Incorporating Visual Experts to Resolve the Information Loss in Multimodal Large Language Models**](https://arxiv.org/abs/2401.03105) <br> | arXiv | 2024-01 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**MM-LLMs: Recent Advances in MultiModal Large Language Models**](https://arxiv.org/abs/2401.13601) <br> | arXiv | 2024-01 | [-](-) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**Q-Instruct: Improving Low-level Visual Abilities for Multi-modality Foundation Models**](https://openaccess.thecvf.com/content/CVPR2024/html/Wu_Q-Instruct_Improving_Low-level_Visual_Abilities_for_Multi-modality_Foundation_Models_CVPR_2024_paper.html) <br> | CVPR | 2024-06 | [Code](https://github.com/Q-Future/Q-Instruct) | - |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback**](https://openaccess.thecvf.com/content/CVPR2024/html/Yu_RLHF-V_Towards_Trustworthy_MLLMs_via_Behavior_Alignment_from_Fine-grained_Correctional_CVPR_2024_paper.html) <br> | CVPR | 2024-06 | [Code](https://rlhf-v.github.io) | - |
| ![Star](https://img.shields.io/github/stars/karpathy/llama2.c.svg?style=social&label=Star) <br> [**Training Compute-Optimal Large Language Models**](https://arxiv.org/abs/2203.15556) <br> | arXiv | 2022-03 | [Code](https://github.com/karpathy/llama2.c) | - |
| ![Star](https://img.shields.io/github/stars/hltchkust/vg-gplms.svg?style=social&label=Star) <br> [**Vision Guided Generative Pre-trained Language Models for Multimodal Abstractive Summarization**](https://arxiv.org/abs/2109.02401) <br> | arXiv | 2021-09 | [Code](https://github.com/hltchkust/vg-gplms) | - |
| ![Star](https://img.shields.io/github/stars/vitron-llm/vitron.svg?style=social&label=Star) <br> [**VITRON: A Unified Pixel-level Vision LLM for Understanding, Generating, Segmenting, Editing**](https://openreview.net/forum?id=kPmSfhCM5s) <br> | OpenReview | 2024 | [Code](https://vitron-llm.github.io/) | - |

### <a id="specific-task">Specific Task</a>

|  Title  |   Venue  |   Date   |   Code   |
|:--------|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**Large Language Models for Lossless Image Compression: Next-Pixel Prediction in Language Space is All You Need**](http://arxiv.org/abs/2411.12448) <br> | arXiv | 2024-11-22 | [-](-) |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**LoMAE: Low-level Vision Masked Autoencoders for Low-dose CT Denoising**](http://arxiv.org/abs/2310.12405) <br> | arXiv | 2023-10-19 | [-](-) |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**DaLPSR: Leverage Degradation-Aligned Language Prompt for Real-World Image Super-Resolution**](http://arxiv.org/abs/2406.16477) <br> | arXiv | 2024-10-19 | [-](-) |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**Star-Net: Improving Single Image Desnowing Model With More Efficient Connection and Diverse Feature Interaction**](http://arxiv.org/abs/2303.09988) <br> | arXiv | 2023-03-17 | [-](-) |

### <a id="multiple-tasks">Multiple Tasks</a>

|  Title  |   Venue  |   Date   |   Code   |
|:--------|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**Learning A Low-Level Vision Generalist via Visual Task Prompt**](http://arxiv.org/abs/2408.08601) <br> | arXiv | 2024-08-16 | [-](-) |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**LM4LV: A Frozen Large Language Model for Low-level Vision Tasks**](http://arxiv.org/abs/2405.15734) <br> | arXiv | 2024-06-11 | [-](-) |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**RestoreAgent: Autonomous Image Restoration Agent via Multimodal Large Language Models**](http://arxiv.org/abs/2407.18035) <br> | arXiv | 2024-07-25 | [-](-) |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**LLMRA: Multi-modal Large Language Model based Restoration Assistant**](http://arxiv.org/abs/2401.11401) <br> | arXiv | 2024-01-21 | [-](-) |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**Beyond Text: Frozen Large Language Models in Visual Signal Comprehension**](http://arxiv.org/abs/2403.07874) <br> | arXiv | 2024-03-12 | [-](-) |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**Controlling Vision-Language Models for Multi-Task Image Restoration**](http://arxiv.org/abs/2310.01018) <br> | arXiv | 2024-02-28 | [-](-) |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**Multimodal Prompt Perceiver: Empower Adaptiveness, Generalizability and Fidelity for All-in-One Image Restoration**](http://arxiv.org/abs/2312.02918) <br> | arXiv | 2024-03-20 | [-](-) |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**Clarity ChatGPT: An Interactive and Adaptive Processing System for Image Restoration and Enhancement**](http://arxiv.org/abs/2311.11695) <br> | arXiv | 2023-11-20 | [-](-) |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**Diff-Restorer: Unleashing Visual Prompts for Diffusion-based Universal Image Restoration**](http://arxiv.org/abs/2407.03636) <br> | arXiv | 2024-07-04 | [-](-) |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**AllRestorer: All-in-One Transformer for Image Restoration under Composite Degradations**](http://arxiv.org/abs/2411.10708) <br> | arXiv | 2024-11-16 | [-](-) |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**GridFormer: Residual Dense Transformer with Grid Structure for Image Restoration in Adverse Weather Conditions**](https://doi.org/10.1007/s11263-024-02056-0) <br> | - | 2024-10-01 | [-](-) |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**Multi-Expert Adaptive Selection: Task-Balancing for All-in-One Image Restoration**](http://arxiv.org/abs/2407.19139) <br> | arXiv | 2024-07-27 | [-](-) |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**Leveraging vision-language prompts for real-world image restoration and enhancement**](https://www.sciencedirect.com/science/article/pii/S1077314224003035) <br> | - | 2025-01-01 | [-](-) |
| ![Star](https://img.shields.io/github/stars/-/-.svg?style=social&label=Star) <br> [**LoRA-IR: Taming Low-Rank Experts for Efficient All-in-One Image Restoration**](http://arxiv.org/abs/2410.15385) <br> | arXiv | 2024-11-16 | [-](-) |

## <a id="related-surveys-recommended">Related Surveys Recommended</a>

**A Survey on Large Language Models for Recommendation**<br />
arXiv 2024. [[Paper](http://arxiv.org/abs/2305.19860)] <br />Jun. 2024<br />

**A Survey of Large Language Models**<br />
arXiv 2024. [[Paper](http://arxiv.org/abs/2303.18223)] <br />Oct. 2024<br />

**From Word Vectors to Multimodal Embeddings: Techniques, Applications, and Future Directions For Large Language Models**<br />
arXiv 2024. [[Paper](http://arxiv.org/abs/2411.05036)] <br />Nov. 2024<br />

**MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs**<br />
arXiv 2024. [[Paper](http://arxiv.org/abs/2411.15296)] <br />Nov. 2024<br />

**Natural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey**<br />
arXiv 2024. [[Paper](http://arxiv.org/abs/2411.17558)] <br />Nov. 2024<br />

**Large Language Model-Brained GUI Agents: A Survey**<br />
arXiv 2024. [[Paper](http://arxiv.org/abs/2411.18279)] <br />Nov. 2024<br />

**Visual Prompting in Multimodal Large Language Models: A Survey**<br />
arXiv 2024. [[Paper](http://arxiv.org/abs/2409.15310)] <br />Sep. 2024<br />

**A Survey of Camouflaged Object Detection and Beyond**<br />
arXiv 2024. [[Paper](http://arxiv.org/abs/2408.14562)] <br />Aug. 2024<br />

**MM-LLMs: Recent Advances in MultiModal Large Language Models**<br />
arXiv 2024. [[Paper](http://arxiv.org/abs/2401.13601)] <br />May. 2024<br />

**A Survey on Benchmarks of Multimodal Large Language Models**<br />
arXiv 2024. [[Paper](http://arxiv.org/abs/2408.08632)] <br />Sep. 2024<br />

**A Survey on Multimodal Benchmarks: In the Era of Large AI Models**<br />
arXiv 2024. [[Paper](http://arxiv.org/abs/2409.18142)] <br />Sep. 2024<br />

**Multimodal Image Synthesis and Editing: The Generative AI Era**<br />
arXiv 2023. [[Paper](http://arxiv.org/abs/2112.13592)] <br />Aug. 2023<br />

**A Survey on Visual Transformer**<br />
arXiv 2023. [[Paper](http://arxiv.org/abs/2012.12556)] <br />Jul. 2023<br />

**Personalized Multimodal Large Language Models: A Survey**<br />
arXiv 2024. [[Paper](http://arxiv.org/abs/2412.02142)] <br />Dec. 2024<br />

**Explainable and Interpretable Multimodal Large Language Models: A Comprehensive Survey**<br />
arXiv 2024. [[Paper](http://arxiv.org/abs/2412.02104)] <br />Dec. 2024<br />

**A Survey on Vision-Language-Action Models for Embodied AI**<br />
arXiv 2024. [[Paper](http://arxiv.org/abs/2405.14093)] <br />Nov. 2024<br />

**A Survey on Multimodal Large Language Models**<br />
National Science Review 2024. [[Paper](https://doi.org/10.1093/nsr/nwae403)] <br />Nov. 2024<br />

**How to Bridge the Gap between Modalities: A Comprehensive Survey on Multi-modal Large Language Model**<br />
arXiv 2023. [[Paper](https://arxiv.org/abs/2311.07594)] <br />Nov. 2023<br />

**Multimodal Large Language Models: A Survey**<br />
IEEE 2023. [[Paper](https://ieeexplore.ieee.org/abstract/document/10386743)] <br />2023<br />

**Multimodal Learning With Transformers: A Survey**<br />
IEEE 2023. [[Paper](https://ieeexplore.ieee.org/abstract/document/10123038)] <br />2023<br />

**Vision-Language Models for Vision Tasks: A Survey**<br />
IEEE 2023. [[Paper](https://ieeexplore.ieee.org/abstract/document/10445007)] [[Code](https://github.com/jingyi0000/vlm_survey)] <br />2023<br />


## <a id="awesome-datasets">Awesome Datasets</a>

### <a id="dehazing">Dehazing</a>

### <a id="deblurring">Deblurring</a>

### <a id="deraining">Deraining</a>

### <a id="desnowing">Desnowing</a>

### <a id="denoising">Denoising</a>

### <a id="other">Other</a>

## <a id="benchmarks-for-evaluation">Benchmarks for Evaluation</a>

|  Name  |   Paper  |   Link   |   Notes   |
|:--------|:--------:|:--------:|:--------:|
| **Q-Bench** | [Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision](http://arxiv.org/abs/2309.14181) | [repo](https://q-future.github.io/Q-Bench) | A holistic benchmark crafted to systematically evaluate potential abilities of MLLMs on three realms: low-level visual perception, low-level visual description, and overall visual quality assessment. |
| **Q-Bench<sup>+</sup>** | [Q-Bench<sup>+</sup>: A Benchmark for Multi-modal Foundation Models on Low-level Vision from Single Images to Pairs](http://arxiv.org/abs/2402.07116) | [repo](https://github.com/Q-Future/Q-Bench) | A benchmark settings to emulate human language responses related to low-level vision: the low-level visual perception via visual question answering related to low-level attributes; and the low-level visual description, on evaluating MLLMs for low-level text descriptions. |
| **HEIE** | [HEIE: MLLM-Based Hierarchical Explainable AIGC Image Implausibility Evaluator](http://arxiv.org/abs/2411.17261) | [repo](-) | A novel MLLM-Based Hierarchical Explainable image Implausibility Evaluator. |
| **QL-Bench** | [Explore the Hallucination on Low-level Perception for MLLMs](http://arxiv.org/abs/2409.09748) | [repo](-) | - |
| **MLLM-as-a-Judge** | [MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark](https://arxiv.org/abs/2402.04788) | [repo](https://github.com/Dongping-Chen/MLLM-Judge) | - |
| **Q-BOOST** | [Q-BOOST: On Visual Quality Assessment Ability of Low-Level Multi-Modality Foundation Models](https://ieeexplore.ieee.org/abstract/document/10645451) | [repo](https://github.com/Q-Future/Q-Instruct/boost_qa) | A focused exploration of the visual quality assessment capabilities in low-level multi-modality foundation models, introducing Q-BOOST as a benchmark framework. |
| **SEED-Bench** | [SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension](https://arxiv.org/abs/2307.16125) | [repo](https://github.com/AILab-CVC/SEED-Bench) | A benchmark designed to evaluate the generative comprehension abilities of multimodal large language models across diverse tasks and datasets. |
| **MIBench** | [MIBench: Evaluating Multimodal Large Language Models over Multiple Images](http://arxiv.org/abs/2407.15272) | [repo](-) | - |
| **AICoderEval** | [AICoderEval: Improving AI Domain Code Generation of Large Language Models](http://arxiv.org/abs/2406.04712) | [repo](-) | - |
| **MLe-Bench** | [MLeVLM: Improve Multi-level Progressive Capabilities based on Multimodal Large Language Model for Medical Visual Question Answering](https://aclanthology.org/2024.findings-acl.296) | [repo](-) | - |

### <a id="metrics">Metrics</a>

## <a id="reference">Reference</a>
[Awesome-Multimodal-Large-Language-Models-by-BradyFU](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)

